import sys
from crawler import AITimesCrawler
from db_manager import PostgresDBManager
from blog_writer import BlogWriter
import psycopg2
import os
from dotenv import load_dotenv

def get_all_companies_from_db():

    load_dotenv()
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        port=os.getenv("DB_PORT"),
        dbname=os.getenv("DB_NAME"),
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD")
    )
    cur = conn.cursor()
    cur.execute("""
        SELECT tablename FROM pg_tables 
        WHERE schemaname = 'public' AND tablename LIKE '%_articles';
    """)
    tables = cur.fetchall()
    cur.close()
    conn.close()

    return [t[0].replace("_articles", "").replace("_", " ").title() for t in tables]

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("â— ì‹¤í–‰ ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤: crawl ë˜ëŠ” blog")
        sys.exit(1)

    mode = sys.argv[1]

    if mode == "crawl":
        crawler = AITimesCrawler()
        db = PostgresDBManager()

        df_all = crawler.crawl(max_pages=3)
        df_filtered = crawler.extract_articles_with_known_companies(df_all)
        df_final = crawler.add_article_content(df_filtered)

        for _, row in df_final.iterrows():
            for company in row["companies"].split(", "):
                db.insert_article_to_company_table(
                    company_name=company,
                    title=row["title"],
                    url=row["link"],
                    content=row["content"],
                    published_at=row["published_at"].replace(".", "-")
                )
        db.close()

    elif mode == "blog":
        if len(sys.argv) < 3:
            print("â— blog ëª¨ë“œì—ì„œëŠ” ê¸°ì—…ëª…ì„ ìµœì†Œ 1ê°œ ì´ìƒ ìž…ë ¥í•˜ê±°ë‚˜ 'all'ì„ ìž…ë ¥í•˜ì„¸ìš”.")
            print("ì˜ˆì‹œ: python main.py blog OpenAI ë„¤ì´ë²„")
            print("ì˜ˆì‹œ: python main.py blog all")
            sys.exit(1)

        if sys.argv[2].lower() == "all":
            companies = get_all_companies_from_db()
        else:
            companies = sys.argv[2:]

        writer = BlogWriter()

        for company in companies:
            while True:
                print(f"ðŸ”Ž {company} ì‚¬ìš©í•˜ì§€ ì•Šì€ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
                articles = writer.fetch_recent_articles(company)
                if len(articles) < 3:
                    print(f"â›” {company} ê¸°ì‚¬ 3ê°œ ë¯¸ë§Œ â†’ ê¸€ ìƒì„± ì¢…ë£Œ\n")
                    break

                print(f"âœï¸ {company} ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì¤‘...")
                blog_text, used_ids, latest_date = writer.generate_blog_post(company, articles)
                filepath = writer.save_to_markdown(company, blog_text, latest_date)
                writer.mark_articles_as_used(company, used_ids)

                print(f"ðŸ“„ ë¸”ë¡œê·¸ ìƒì„± ì™„ë£Œ â†’ {filepath}\n")

        writer.close()

    else:
        print("â— ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œìž…ë‹ˆë‹¤. ì‚¬ìš©ë²•: python main.py [crawl|blog íšŒì‚¬1 íšŒì‚¬2 ...|blog all]")
