import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

AI_COMPANY_ALIASES = {
    "OpenAI": ["OpenAI", "ì˜¤í”ˆAI", "ì˜¤í”ˆì—ì´ì•„ì´"],
    "Google": ["Google", "êµ¬ê¸€"],
    "DeepMind": ["DeepMind", "ë”¥ë§ˆì¸ë“œ"],
    "Microsoft": ["Microsoft", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "MS"],
    "Amazon": ["Amazon", "ì•„ë§ˆì¡´"],
    "Meta": ["Meta", "ë©”íƒ€", "í˜ì´ìŠ¤ë¶"],
    "NVIDIA": ["NVIDIA", "ì—”ë¹„ë””ì•„"],
    "Anthropic": ["Anthropic", "ì•¤íŠ¸ë¡œí”½"],
    "Mistral": ["Mistral", "ë¯¸ìŠ¤íŠ¸ë„"],
    "xAI": ["xAI", "ì¼ë¡  ë¨¸ìŠ¤í¬ AI", "ì¼ë¡ ë¨¸ìŠ¤í¬ AI"],
    "Cohere": ["Cohere", "ì½”íˆì–´"],
    "Hugging Face": ["Hugging Face", "í—ˆê¹…í˜ì´ìŠ¤"],
    "Stability AI": ["Stability AI", "ìŠ¤í…Œë¹Œë¦¬í‹°AI"],
    "ë„¤ì´ë²„": ["ë„¤ì´ë²„", "Naver", "í•˜ì´í¼í´ë¡œë°”"],
    "ì¹´ì¹´ì˜¤": ["ì¹´ì¹´ì˜¤", "Kakao"],
    "ë¤¼íŠ¼": ["ë¤¼íŠ¼", "Riiid", "Riiid!"],
    "ì—…ìŠ¤í…Œì´ì§€": ["ì—…ìŠ¤í…Œì´ì§€", "Upstage"],
    "í‹°ì“°ë¦¬í": ["T3Q", "í‹°ì“°ë¦¬í"],
    "ì†”íŠ¸ë£©ìŠ¤": ["ì†”íŠ¸ë£©ìŠ¤", "Saltlux"],
}

class AITimesCrawler:
    def __init__(self):
        self.results = []

        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        self.driver = webdriver.Chrome(options=options)

    def crawl(self, max_pages=3):
        print("â–¶ï¸ AIíƒ€ì„ì¦ˆ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
        for page in range(1, max_pages + 1):
            url = f"https://www.aitimes.com/news/articleList.html?page={page}&view_type=sm"
            print(f"ğŸ“„ í˜ì´ì§€ {page}: {url}")
            self.driver.get(url)
            time.sleep(2)

            try:
                ul = self.driver.find_element(By.CSS_SELECTOR, "ul.type2")
                li_list = ul.find_elements(By.TAG_NAME, "li")

                for li in li_list:
                    try:
                        title_tag = li.find_element(By.CSS_SELECTOR, "h4.titles > a")
                        title = title_tag.text.strip()
                        link = title_tag.get_attribute("href")

                        if link.startswith("/"):
                            link = "https://www.aitimes.com" + link

                        # âœ… ë°œí–‰ì¼ ìˆ˜ì§‘ (ì‹œê°„ ì œê±°)
                        byline = li.find_element(By.CLASS_NAME, "byline")
                        em_tags = byline.find_elements(By.TAG_NAME, "em")
                        full_date = em_tags[1].text.strip() if len(em_tags) > 1 else ""
                        published_date = full_date.split()[0] if full_date else ""

                        if title and link:
                            self.results.append({
                                "title": title,
                                "link": link,
                                "published_at": published_date
                            })
                    except:
                        continue
            except Exception as e:
                print("âŒ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨:", e)

        self.driver.quit()
        return pd.DataFrame(self.results)

    def extract_articles_with_known_companies(self, df):
        print("ğŸ” ê¸°ì—…ëª… í•„í„°ë§ ì¤‘...")
        filtered = []
        
        for _, row in df.iterrows():
            title = row["title"]
            matched = []
            for company, aliases in AI_COMPANY_ALIASES.items():
                if any(alias in title for alias in aliases):
                    matched.append(company)
                    
            if matched:
                filtered.append({
                    "title": title,
                    "link": row["link"],
                    "published_at": row["published_at"],
                    "companies": ", ".join(set(matched))
                })
                
        return pd.DataFrame(filtered)

    def add_article_content(self, df):
        print("ğŸ“ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
        contents = []
        for _, row in df.iterrows():
            content = self.get_article_content(row["link"])
            contents.append(content)
        df["content"] = contents
        return df

    def get_article_content(self, url):
        try:
            headers = {
                "User-Agent": "Mozilla/5.0"
            }
            response = requests.get(url, headers=headers, timeout=7)
            soup = BeautifulSoup(response.text, "lxml")

            body = soup.find("article", id="article-view-content-div")
            return body.get_text(strip=True) if body else ""
        except Exception as e:
            print(f"âŒ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            return ""
